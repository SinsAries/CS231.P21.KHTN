{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11916287,"sourceType":"datasetVersion","datasetId":7491295},{"sourceId":406203,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":331925,"modelId":352809},{"sourceId":406331,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":332029,"modelId":352932},{"sourceId":406975,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":332536,"modelId":353470},{"sourceId":407780,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":333198,"modelId":354135},{"sourceId":407930,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":333318,"modelId":354277},{"sourceId":408138,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":333462,"modelId":354461},{"sourceId":414499,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":338343,"modelId":359290},{"sourceId":415127,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":338751,"modelId":359796},{"sourceId":417717,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":340742,"modelId":361988},{"sourceId":421223,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":343289,"modelId":364572},{"sourceId":421251,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":343309,"modelId":364591},{"sourceId":421253,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":343310,"modelId":364592},{"sourceId":421255,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":343312,"modelId":364594},{"sourceId":421258,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":343314,"modelId":364596}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install zetascale\n!pip install swarms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T21:26:10.347989Z","iopub.execute_input":"2025-06-01T21:26:10.348643Z","iopub.status.idle":"2025-06-01T21:28:01.313856Z","shell.execute_reply.started":"2025-06-01T21:26:10.348609Z","shell.execute_reply":"2025-06-01T21:28:01.313136Z"}},"outputs":[{"name":"stdout","text":"Collecting zetascale\n  Downloading zetascale-2.8.6-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from zetascale) (1.5.2)\nCollecting argparse<2.0.0,>=1.4.0 (from zetascale)\n  Downloading argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: beartype in /usr/local/lib/python3.11/dist-packages (from zetascale) (0.20.2)\nCollecting bitsandbytes (from zetascale)\n  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nCollecting colt5-attention (from zetascale)\n  Downloading CoLT5_attention-0.11.1-py3-none-any.whl.metadata (737 bytes)\nRequirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from zetascale) (0.8.1)\nCollecting einops-exts==0.0.4 (from zetascale)\n  Downloading einops_exts-0.0.4-py3-none-any.whl.metadata (621 bytes)\nCollecting joblib<1.4.0,>=1.3.0 (from zetascale)\n  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\nCollecting local-attention (from zetascale)\n  Downloading local_attention-1.11.1-py3-none-any.whl.metadata (907 bytes)\nCollecting loguru (from zetascale)\n  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from zetascale) (14.0.0)\nCollecting scikit-learn<1.6.0,>=1.5.0 (from zetascale)\n  Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from zetascale) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from zetascale) (0.21.0+cu124)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from zetascale) (4.67.1)\nRequirement already satisfied: transformers<5.0.0,>=4.20.0 in /usr/local/lib/python3.11/dist-packages (from zetascale) (4.51.3)\nCollecting vector-quantize-pytorch (from zetascale)\n  Downloading vector_quantize_pytorch-1.22.16-py3-none-any.whl.metadata (30 kB)\nRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.6.0,>=1.5.0->zetascale) (1.26.4)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.6.0,>=1.5.0->zetascale) (1.15.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.6.0,>=1.5.0->zetascale) (3.6.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.20.0->zetascale) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.20.0->zetascale) (0.31.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.20.0->zetascale) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.20.0->zetascale) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.20.0->zetascale) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.20.0->zetascale) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.20.0->zetascale) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.20.0->zetascale) (0.5.3)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate->zetascale) (7.0.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->zetascale) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->zetascale) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->zetascale) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->zetascale) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->zetascale) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->zetascale) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->zetascale) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch->zetascale)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch->zetascale)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch->zetascale)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch->zetascale)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch->zetascale)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch->zetascale)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->zetascale) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->zetascale) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->zetascale) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch->zetascale)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->zetascale) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->zetascale) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->zetascale) (1.3.0)\nCollecting hyper-connections>=0.1.8 (from local-attention->zetascale)\n  Downloading hyper_connections-0.1.15-py3-none-any.whl.metadata (5.2 kB)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->zetascale) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->zetascale) (2.19.1)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->zetascale) (11.1.0)\nCollecting einx>=0.3.0 (from vector-quantize-pytorch->zetascale)\n  Downloading einx-0.3.0-py3-none-any.whl.metadata (6.9 kB)\nRequirement already satisfied: frozendict in /usr/local/lib/python3.11/dist-packages (from einx>=0.3.0->vector-quantize-pytorch->zetascale) (2.4.6)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers<5.0.0,>=4.20.0->zetascale) (1.1.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->zetascale) (0.1.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn<1.6.0,>=1.5.0->zetascale) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn<1.6.0,>=1.5.0->zetascale) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn<1.6.0,>=1.5.0->zetascale) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn<1.6.0,>=1.5.0->zetascale) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn<1.6.0,>=1.5.0->zetascale) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn<1.6.0,>=1.5.0->zetascale) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->zetascale) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers<5.0.0,>=4.20.0->zetascale) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers<5.0.0,>=4.20.0->zetascale) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers<5.0.0,>=4.20.0->zetascale) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers<5.0.0,>=4.20.0->zetascale) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn<1.6.0,>=1.5.0->zetascale) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn<1.6.0,>=1.5.0->zetascale) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.5->scikit-learn<1.6.0,>=1.5.0->zetascale) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.5->scikit-learn<1.6.0,>=1.5.0->zetascale) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.5->scikit-learn<1.6.0,>=1.5.0->zetascale) (2024.2.0)\nDownloading zetascale-2.8.6-py3-none-any.whl (533 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m533.8/533.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading einops_exts-0.0.4-py3-none-any.whl (3.9 kB)\nDownloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\nDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading CoLT5_attention-0.11.1-py3-none-any.whl (18 kB)\nDownloading local_attention-1.11.1-py3-none-any.whl (9.4 kB)\nDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading vector_quantize_pytorch-1.22.16-py3-none-any.whl (47 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading einx-0.3.0-py3-none-any.whl (102 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading hyper_connections-0.1.15-py3-none-any.whl (15 kB)\nInstalling collected packages: argparse, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, loguru, joblib, einops-exts, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, hyper-connections, local-attention, colt5-attention, einx, vector-quantize-pytorch, scikit-learn, bitsandbytes, zetascale\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: joblib\n    Found existing installation: joblib 1.5.0\n    Uninstalling joblib-1.5.0:\n      Successfully uninstalled joblib-1.5.0\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed argparse-1.4.0 bitsandbytes-0.46.0 colt5-attention-0.11.1 einops-exts-0.0.4 einx-0.3.0 hyper-connections-0.1.15 joblib-1.3.2 local-attention-1.11.1 loguru-0.7.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 scikit-learn-1.5.2 vector-quantize-pytorch-1.22.16 zetascale-2.8.6\nCollecting swarms\n  Downloading swarms-7.8.0-py3-none-any.whl.metadata (94 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.0/95.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from swarms) (6.0.2)\nRequirement already satisfied: aiofiles in /usr/local/lib/python3.11/dist-packages (from swarms) (22.1.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from swarms) (3.11.18)\nCollecting asyncio<4.0,>=3.4.3 (from swarms)\n  Downloading asyncio-3.4.3-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: docstring_parser==0.16 in /usr/local/lib/python3.11/dist-packages (from swarms) (0.16)\nRequirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from swarms) (0.28.1)\nCollecting litellm (from swarms)\n  Downloading litellm-1.72.0-py3-none-any.whl.metadata (39 kB)\nRequirement already satisfied: loguru in /usr/local/lib/python3.11/dist-packages (from swarms) (0.7.3)\nCollecting mcp (from swarms)\n  Downloading mcp-1.9.2-py3-none-any.whl.metadata (28 kB)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from swarms) (3.4.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from swarms) (1.26.4)\nCollecting numpydoc (from swarms)\n  Downloading numpydoc-1.8.0-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from swarms) (7.0.0)\nRequirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from swarms) (2.11.4)\nCollecting pypdf==5.1.0 (from swarms)\n  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\nCollecting python-dotenv (from swarms)\n  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from swarms) (14.0.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from swarms) (75.2.0)\nRequirement already satisfied: tenacity in /usr/local/lib/python3.11/dist-packages (from swarms) (9.1.2)\nRequirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from swarms) (0.10.2)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from swarms) (2.6.0+cu124)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->swarms) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->swarms) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->swarms) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->swarms) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->swarms) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->swarms) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->swarms) (1.20.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->swarms) (4.9.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->swarms) (2025.4.26)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->swarms) (1.0.7)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->swarms) (3.10)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->swarms) (0.14.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from litellm->swarms) (8.1.8)\nRequirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.11/dist-packages (from litellm->swarms) (8.7.0)\nRequirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from litellm->swarms) (3.1.6)\nRequirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from litellm->swarms) (4.23.0)\nRequirement already satisfied: openai>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from litellm->swarms) (1.70.0)\nRequirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from litellm->swarms) (0.9.0)\nRequirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from litellm->swarms) (0.21.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->swarms) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->swarms) (2.33.2)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->swarms) (4.13.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->swarms) (0.4.0)\nCollecting httpx-sse>=0.4 (from mcp->swarms)\n  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\nCollecting pydantic-settings>=2.5.2 (from mcp->swarms)\n  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\nCollecting python-multipart>=0.0.9 (from mcp->swarms)\n  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\nCollecting sse-starlette>=1.6.1 (from mcp->swarms)\n  Downloading sse_starlette-2.3.6-py3-none-any.whl.metadata (10 kB)\nCollecting starlette>=0.27 (from mcp->swarms)\n  Downloading starlette-0.47.0-py3-none-any.whl.metadata (6.2 kB)\nCollecting uvicorn>=0.23.1 (from mcp->swarms)\n  Downloading uvicorn-0.34.3-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->swarms) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->swarms) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->swarms) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->swarms) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->swarms) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->swarms) (2.4.1)\nRequirement already satisfied: sphinx>=6 in /usr/local/lib/python3.11/dist-packages (from numpydoc->swarms) (8.2.3)\nRequirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.11/dist-packages (from numpydoc->swarms) (0.9.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->swarms) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->swarms) (2.19.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->swarms) (3.18.0)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->swarms) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->swarms) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->swarms) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->swarms) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->swarms) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->swarms) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->swarms) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->swarms) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->swarms) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->swarms) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->swarms) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->swarms) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->swarms) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->swarms) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->swarms) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->swarms) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->swarms) (1.3.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->swarms) (1.3.1)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.8.0->litellm->swarms) (3.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm->swarms) (3.0.2)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm->swarms) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm->swarms) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm->swarms) (0.24.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->swarms) (0.1.2)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.68.2->litellm->swarms) (1.9.0)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.68.2->litellm->swarms) (0.9.0)\nRequirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai>=1.68.2->litellm->swarms) (4.67.1)\nRequirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx>=6->numpydoc->swarms) (2.0.0)\nRequirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=6->numpydoc->swarms) (2.0.0)\nRequirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=6->numpydoc->swarms) (2.1.0)\nRequirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx>=6->numpydoc->swarms) (1.0.1)\nRequirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=6->numpydoc->swarms) (2.0.0)\nRequirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx>=6->numpydoc->swarms) (2.0.0)\nRequirement already satisfied: docutils<0.22,>=0.20 in /usr/local/lib/python3.11/dist-packages (from sphinx>=6->numpydoc->swarms) (0.21.2)\nRequirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx>=6->numpydoc->swarms) (2.2.0)\nRequirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx>=6->numpydoc->swarms) (2.17.0)\nRequirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx>=6->numpydoc->swarms) (1.0.0)\nRequirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx>=6->numpydoc->swarms) (1.4.1)\nRequirement already satisfied: requests>=2.30.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=6->numpydoc->swarms) (2.32.3)\nRequirement already satisfied: roman-numerals-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=6->numpydoc->swarms) (3.1.0)\nRequirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=6->numpydoc->swarms) (25.0)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.7.0->litellm->swarms) (2024.11.6)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->swarms) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->swarms) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->swarms) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->swarms) (2024.2.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers->litellm->swarms) (0.31.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm->swarms) (1.1.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->swarms) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=6->numpydoc->swarms) (3.4.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=6->numpydoc->swarms) (2.4.0)\nDownloading swarms-7.8.0-py3-none-any.whl (510 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.2/510.2 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading asyncio-3.4.3-py3-none-any.whl (101 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading litellm-1.72.0-py3-none-any.whl (8.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\nDownloading mcp-1.9.2-py3-none-any.whl (131 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpydoc-1.8.0-py3-none-any.whl (64 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\nDownloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\nDownloading sse_starlette-2.3.6-py3-none-any.whl (10 kB)\nDownloading starlette-0.47.0-py3-none-any.whl (72 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.8/72.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading uvicorn-0.34.3-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: asyncio, uvicorn, python-multipart, python-dotenv, pypdf, httpx-sse, starlette, sse-starlette, pydantic-settings, numpydoc, mcp, litellm, swarms\n  Attempting uninstall: pypdf\n    Found existing installation: pypdf 5.4.0\n    Uninstalling pypdf-5.4.0:\n      Successfully uninstalled pypdf-5.4.0\nSuccessfully installed asyncio-3.4.3 httpx-sse-0.4.0 litellm-1.72.0 mcp-1.9.2 numpydoc-1.8.0 pydantic-settings-2.9.1 pypdf-5.1.0 python-dotenv-1.1.0 python-multipart-0.0.20 sse-starlette-2.3.6 starlette-0.47.0 swarms-7.8.0 uvicorn-0.34.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\nfrom torch import nn, Tensor\nfrom zeta.nn import SSM\nfrom einops.layers.torch import Reduce","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-01T21:28:01.315409Z","iopub.execute_input":"2025-06-01T21:28:01.315694Z","iopub.status.idle":"2025-06-01T21:28:41.550515Z","shell.execute_reply.started":"2025-06-01T21:28:01.315650Z","shell.execute_reply":"2025-06-01T21:28:41.549726Z"}},"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748813298.550892      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748813298.668791      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Modules","metadata":{}},{"cell_type":"code","source":"# Pair\ndef pair(t):\n    return t if isinstance(t, tuple) else (t, t)\n\n\ndef output_head(dim: int, num_classes: int):\n    \"\"\"\n    Creates a head for the output layer of a model.\n\n    Args:\n        dim (int): The input dimension of the head.\n        num_classes (int): The number of output classes.\n\n    Returns:\n        nn.Sequential: The output head module.\n    \"\"\"\n    return nn.Sequential(\n        Reduce(\"b s d -> b d\", \"mean\"),\n        nn.LayerNorm(dim),\n        nn.Linear(dim, num_classes),\n    )\n\n\nclass VisionEncoderMambaBlock(nn.Module):\n    \"\"\"\n    VisionMambaBlock is a module that implements the Mamba block from the paper\n    Vision Mamba: Efficient Visual Representation Learning with Bidirectional\n    State Space Model\n\n    Args:\n        dim (int): The input dimension of the input tensor.\n        dt_rank (int): The rank of the state space model.\n        dim_inner (int): The dimension of the inner layer of the\n            multi-head attention.\n        d_state (int): The dimension of the state space model.\n\n\n    Example:\n    >>> block = VisionMambaBlock(dim=256, heads=8, dt_rank=32,\n            dim_inner=512, d_state=256)\n    >>> x = torch.randn(1, 32, 256)\n    >>> out = block(x)\n    >>> out.shape\n    torch.Size([1, 32, 256])\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        dt_rank: int,\n        dim_inner: int,\n        d_state: int,\n    ):\n        super().__init__()\n        self.dim = dim\n        self.dt_rank = dt_rank\n        self.dim_inner = dim_inner\n        self.d_state = d_state\n\n        self.forward_conv1d = nn.Conv1d(\n            in_channels=dim, out_channels=dim, kernel_size=1\n        )\n        self.backward_conv1d = nn.Conv1d(\n            in_channels=dim, out_channels=dim, kernel_size=1\n        )\n        self.norm = nn.LayerNorm(dim)\n        self.silu = nn.SiLU()\n        self.ssm = SSM(dim, dt_rank, dim_inner, d_state)\n\n        # Linear layer for z and x\n        self.proj = nn.Linear(dim, dim)\n\n        # Softplus\n        self.softplus = nn.Softplus()\n\n    def forward(self, x: torch.Tensor):\n        b, s, d = x.shape\n\n        # Skip connection\n        skip = x\n\n        # Normalization\n        x = self.norm(x)\n\n        # Split x into x1 and x2 with linears\n        z1 = self.proj(x)\n        x = self.proj(x)\n\n        # forward con1d\n        x1 = self.process_direction(\n            x,\n            self.forward_conv1d,\n            self.ssm,\n        )\n\n        # backward conv1d\n        x2 = self.process_direction(\n            x,\n            self.backward_conv1d,\n            self.ssm,\n        )\n\n        # Activation\n        z = self.silu(z1)\n\n        # Matmul\n        x1 *= z\n        x2 *= z\n\n        # Residual connection\n        return x1 + x2 + skip\n\n    def process_direction(\n        self,\n        x: Tensor,\n        conv1d: nn.Conv1d,\n        ssm: SSM,\n    ):\n        x = rearrange(x, \"b s d -> b d s\")\n        x = self.softplus(conv1d(x))\n        # print(f\"Conv1d: {x}\")\n        x = rearrange(x, \"b d s -> b s d\")\n        x = ssm(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T21:28:41.551433Z","iopub.execute_input":"2025-06-01T21:28:41.552075Z","iopub.status.idle":"2025-06-01T21:28:41.561333Z","shell.execute_reply.started":"2025-06-01T21:28:41.552048Z","shell.execute_reply":"2025-06-01T21:28:41.560711Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class Vim(nn.Module):\n    \"\"\"\n    Vision Mamba (Vim) model implementation.\n\n    Args:\n        dim (int): Dimension of the model.\n        dt_rank (int, optional): Rank of the dynamic tensor. Defaults to 32.\n        dim_inner (int, optional): Inner dimension of the model. Defaults to None.\n        d_state (int, optional): State dimension of the model. Defaults to None.\n        num_classes (int, optional): Number of output classes. Defaults to None.\n        image_size (int, optional): Size of the input image. Defaults to 224.\n        patch_size (int, optional): Size of the image patch. Defaults to 16.\n        channels (int, optional): Number of image channels. Defaults to 3.\n        dropout (float, optional): Dropout rate. Defaults to 0.1.\n        depth (int, optional): Number of encoder layers. Defaults to 12.\n\n    Attributes:\n        dim (int): Dimension of the model.\n        dt_rank (int): Rank of the dynamic tensor.\n        dim_inner (int): Inner dimension of the model.\n        d_state (int): State dimension of the model.\n        num_classes (int): Number of output classes.\n        image_size (int): Size of the input image.\n        patch_size (int): Size of the image patch.\n        channels (int): Number of image channels.\n        dropout (float): Dropout rate.\n        depth (int): Number of encoder layers.\n        to_patch_embedding (nn.Sequential): Sequential module for patch embedding.\n        dropout (nn.Dropout): Dropout module.\n        cls_token (nn.Parameter): Class token parameter.\n        to_latent (nn.Identity): Identity module for latent representation.\n        layers (nn.ModuleList): List of encoder layers.\n        output_head (output_head): Output head module.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        dt_rank: int = 32,\n        dim_inner: int = None,\n        d_state: int = None,\n        num_classes: int = None,\n        image_size: int = 224,\n        patch_size: int = 16,\n        channels: int = 3,\n        dropout: float = 0.1,\n        depth: int = 12,\n        *args,\n        **kwargs,\n    ):\n        super().__init__()\n        self.dim = dim\n        self.dt_rank = dt_rank\n        self.dim_inner = dim_inner\n        self.d_state = d_state\n        self.num_classes = num_classes\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.channels = channels\n        self.dropout = dropout\n        self.depth = depth\n\n        image_height, image_width = pair(image_size)\n        patch_height, patch_width = pair(patch_size)\n        patch_dim = channels * patch_height * patch_width\n\n        self.to_patch_embedding = nn.Sequential(\n            Rearrange(\n                \"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\",\n                p1=patch_height,\n                p2=patch_height,\n            ),\n            nn.Linear(patch_dim, dim),\n        )\n\n        # Dropout\n        self.dropout = nn.Dropout(dropout)\n\n        # class token\n        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n\n        # Latent\n        self.to_latent = nn.Identity()\n\n        # encoder layers\n        self.layers = nn.ModuleList()\n\n        # Append the encoder layers\n        for _ in range(depth):\n            self.layers.append(\n                VisionEncoderMambaBlock(\n                    dim=dim,\n                    dt_rank=dt_rank,\n                    dim_inner=dim_inner,\n                    d_state=d_state,\n                    *args,\n                    **kwargs,\n                )\n            )\n\n        # Output head\n        self.output_head = output_head(dim, num_classes)\n\n    def forward(self, x: Tensor):\n        # Patch embedding\n        b, c, h, w = x.shape\n\n        x = self.to_patch_embedding(x)\n        # print(f\"Patch embedding: {x.shape}\")\n\n        # Shape\n        b, n, _ = x.shape\n\n        # Cls tokens\n        cls_tokens = repeat(self.cls_token, \"() n d -> b n d\", b=b)\n        # print(f\"Cls tokens: {cls_tokens.shape}\")\n\n        # Concatenate\n        # x = torch.cat((cls_tokens, x), dim=1)\n\n        # Dropout\n        x = self.dropout(x)\n        # print(x.shape)\n\n        # Forward pass with the layers\n        for layer in self.layers:\n            x = layer(x)\n            # print(f\"Layer: {x.shape}\")\n\n        # Latent\n        x = self.to_latent(x)\n\n        # x = reduce(x, \"b s d -> b d\", \"mean\")\n\n        # Output head with the cls tokens\n        return self.output_head(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T21:28:41.563625Z","iopub.execute_input":"2025-06-01T21:28:41.563955Z","iopub.status.idle":"2025-06-01T21:28:41.597553Z","shell.execute_reply.started":"2025-06-01T21:28:41.563927Z","shell.execute_reply":"2025-06-01T21:28:41.596830Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Save & Load checkpoint","metadata":{}},{"cell_type":"code","source":"import torch\nimport os\n\n# Đường dẫn lưu checkpoint\ncheckpoint_path = \"/kaggle/working/model_checkpoint.pth\"\ncheckpoint_path_best = \"/kaggle/working/model_bestcheckpoint.pth\"\n\n# Hàm lưu checkpoint\ndef save_checkpoint(model, optimizer, epoch, loss, scheduler, checkpoint_path):\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict(),\n        'loss': loss\n    }\n    torch.save(checkpoint, checkpoint_path)\n    print(f\"Checkpoint saved at {checkpoint_path}\")\n    \n# Hàm load checkpoint\ndef load_checkpoint(model, optimizer, scheduler, checkpoint_path):\n    if os.path.exists(checkpoint_path):\n        checkpoint = torch.load(checkpoint_path, weights_only=True)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        epoch = checkpoint['epoch']\n        loss = checkpoint['loss']\n        print(f\"Checkpoint loaded! Resuming from Epoch {epoch + 1}, Loss: {loss}\")\n        return model, optimizer, scheduler, epoch + 1, loss\n    else:\n        print(\"No checkpoint found, starting from scratch.\")\n        return model, optimizer, scheduler, 0, None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T21:28:41.598505Z","iopub.execute_input":"2025-06-01T21:28:41.598790Z","iopub.status.idle":"2025-06-01T21:28:41.622625Z","shell.execute_reply.started":"2025-06-01T21:28:41.598769Z","shell.execute_reply":"2025-06-01T21:28:41.621983Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Data Setup","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import random_split, DataLoader\n\n# Data transforms for CIFAR10.\ntransform_train = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    transforms.RandomErasing(p=0.2),\n])\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\n# # Transforms cho CIFAR-100\n# transform_train = transforms.Compose([\n#     transforms.RandomCrop(32, padding=4),\n#     transforms.RandomHorizontalFlip(),\n#     transforms.ToTensor(),\n#     transforms.Normalize(mean=[0.5071, 0.4867, 0.4408],\n#                          std=[0.2675, 0.2565, 0.2761]),\n#     transforms.RandomErasing(p=0.2),\n# ])\n# transform_test = transforms.Compose([\n#     transforms.ToTensor(),\n#     transforms.Normalize(mean=[0.5071, 0.4867, 0.4408],\n#                          std=[0.2675, 0.2565, 0.2761])\n# ])\n\n# Load CIFAR-10\ntrain_dataset = torchvision.datasets.CIFAR10(\n    root='./data', train=True, download=True, transform=transform_train)\n\ntest_dataset = torchvision.datasets.CIFAR10(\n    root='./data', train=False, download=True, transform=transform_test)\n\nshuffle_gen = torch.Generator()\nshuffle_gen.manual_seed(42)\n\n# Dataloaders\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n                          generator=shuffle_gen, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T21:28:41.623377Z","iopub.execute_input":"2025-06-01T21:28:41.623579Z","iopub.status.idle":"2025-06-01T21:28:47.239843Z","shell.execute_reply.started":"2025-06-01T21:28:41.623562Z","shell.execute_reply":"2025-06-01T21:28:47.239187Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 170M/170M [00:02<00:00, 81.6MB/s] \n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Train Model","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef train_model(model, train_loader, test_loader, optimizer, scheduler, \n                criterion, start_epoch, num_epochs, device, model_name):\n    best_acc = 0.0\n    \n    for epoch in range(start_epoch, num_epochs):\n        # Training phase.\n        model.train()\n        running_loss = 0.0\n        train_correct = 0\n        train_total = 0\n        train_loader_tqdm = tqdm(train_loader, desc=f\"{model_name} Epoch {epoch+1}/{num_epochs} - Training\")\n        for inputs, labels in train_loader_tqdm:\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            \n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * inputs.size(0)\n            _, predicted = outputs.max(1)\n            train_total += labels.size(0)\n            train_correct += predicted.eq(labels).sum().item()\n            train_loader_tqdm.set_postfix(loss=running_loss/train_total, acc=100.*train_correct/train_total)\n\n        \n        epoch_train_loss = running_loss / len(train_loader.dataset)\n        epoch_train_acc = 100. * train_correct / train_total\n        print(f\"\\n{model_name} Epoch [{epoch+1}/{num_epochs}] Training Loss: {epoch_train_loss:.4f} | Accuracy: {epoch_train_acc:.2f}%\")\n\n        current_lr = optimizer.param_groups[0]['lr']\n\n        \n        # Validation phase.\n        model.eval()\n        test_loss = 0.0\n        correct = 0\n        total = 0\n        test_loader_tqdm = tqdm(test_loader, desc=f\"{model_name} Epoch {epoch+1}/{num_epochs} - Validation\")\n        with torch.no_grad():\n            for inputs, labels in test_loader_tqdm:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                test_loss += loss.item() * inputs.size(0)\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n                test_loader_tqdm.set_postfix(loss=test_loss/total, acc=100.*correct/total)\n        \n        val_loss = test_loss / len(test_loader.dataset)\n        val_acc = 100. * correct / total\n        print(f\"{model_name} Epoch [{epoch+1}/{num_epochs}] Validation Loss: {val_loss:.4f} | Accuracy: {val_acc:.2f}%\\n\")\n        \n        # wandb.log({\n        #     \"epoch\": epoch,\n        #     \"train_loss\": epoch_train_loss,\n        #     \"train_acc\": epoch_train_acc,\n        #     \"val_loss\": val_loss,\n        #     \"val_acc\": val_acc,\n        #     \"learning_rate\": current_lr\n        # })\n                \n        scheduler.step()\n\n        save_checkpoint(model, optimizer, epoch, loss, scheduler, checkpoint_path)\n        print(f\"Checkpoint saved at epoch {epoch+1}, accuracy: {val_acc:.2f}%\")\n        \n        if val_acc > best_acc:\n            best_acc = val_acc\n            save_checkpoint(model, optimizer, epoch, loss, scheduler, checkpoint_path_best)\n            print(f\"✅ Best checkpoint saved at epoch {epoch+1}, accuracy: {val_acc:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T21:28:47.240601Z","iopub.execute_input":"2025-06-01T21:28:47.240834Z","iopub.status.idle":"2025-06-01T21:28:47.250781Z","shell.execute_reply.started":"2025-06-01T21:28:47.240817Z","shell.execute_reply":"2025-06-01T21:28:47.250063Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import torch.optim as optim\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n    \nVisionMamba_model = Vim(\n                        image_size=32, \n                        patch_size=4, \n                        depth=6, \n                        dim=192, \n                        channels=3, \n                        num_classes=10,\n                        dropout=0.1,\n                        dt_rank=8,\n                        d_state=256,\n                        dim_inner=192\n                    ).to(device)\n\nprint(\"Available GPUs:\", torch.cuda.device_count())\nif torch.cuda.device_count() > 1:\n    VisionMamba_model = nn.DataParallel(VisionMamba_model)\n\n# Number of Trainable parameters\ntrainable_params = sum(p.numel() for p in VisionMamba_model.parameters() if p.requires_grad)\nprint(f\"Trainable parameters: {trainable_params}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T21:28:47.251755Z","iopub.execute_input":"2025-06-01T21:28:47.251946Z","iopub.status.idle":"2025-06-01T21:28:47.534146Z","shell.execute_reply.started":"2025-06-01T21:28:47.251930Z","shell.execute_reply":"2025-06-01T21:28:47.533474Z"}},"outputs":[{"name":"stdout","text":"Available GPUs: 2\nTrainable parameters: 1586698\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"lr = 7e-4\nweight_decay=0.01\nnum_epochs = 200\nstart_epoch = 0\n\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.2)\noptimizer = torch.optim.AdamW(VisionMamba_model.parameters(), lr=lr, weight_decay=weight_decay)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=5e-6)\n\nprev_checkpoint = \"/kaggle/input/visionmambademo13/pytorch/default/1/model_checkpoint.pth\"\nVisionMamba_model, optimizer, scheduler, start_epoch, last_loss = load_checkpoint(VisionMamba_model, optimizer, scheduler, prev_checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T21:28:47.534869Z","iopub.execute_input":"2025-06-01T21:28:47.535151Z","iopub.status.idle":"2025-06-01T21:28:47.858789Z","shell.execute_reply.started":"2025-06-01T21:28:47.535131Z","shell.execute_reply":"2025-06-01T21:28:47.857737Z"}},"outputs":[{"name":"stdout","text":"Checkpoint loaded! Resuming from Epoch 117, Loss: 1.417494535446167\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"print(VisionMamba_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T21:28:47.862051Z","iopub.execute_input":"2025-06-01T21:28:47.862368Z","iopub.status.idle":"2025-06-01T21:28:47.869011Z","shell.execute_reply.started":"2025-06-01T21:28:47.862342Z","shell.execute_reply":"2025-06-01T21:28:47.867455Z"}},"outputs":[{"name":"stdout","text":"DataParallel(\n  (module): Vim(\n    (to_patch_embedding): Sequential(\n      (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=4, p2=4)\n      (1): Linear(in_features=48, out_features=192, bias=True)\n    )\n    (dropout): Dropout(p=0.1, inplace=False)\n    (to_latent): Identity()\n    (layers): ModuleList(\n      (0-5): 6 x VisionEncoderMambaBlock(\n        (forward_conv1d): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n        (backward_conv1d): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n        (silu): SiLU()\n        (ssm): SSM(\n          (deltaBC_layer): Linear(in_features=192, out_features=520, bias=False)\n          (dt_proj_layer): Linear(in_features=8, out_features=192, bias=True)\n        )\n        (proj): Linear(in_features=192, out_features=192, bias=True)\n        (softplus): Softplus(beta=1.0, threshold=20.0)\n      )\n    )\n    (output_head): Sequential(\n      (0): Reduce('b s d -> b d', 'mean')\n      (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n      (2): Linear(in_features=192, out_features=10, bias=True)\n    )\n  )\n)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Train the model.\ntrain_model(VisionMamba_model, train_loader, test_loader, optimizer, scheduler, criterion,\n        start_epoch, num_epochs, device, model_name=\"VisionMamba\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T21:28:47.870038Z","iopub.execute_input":"2025-06-01T21:28:47.870365Z","execution_failed":"2025-06-02T02:58:55.130Z"}},"outputs":[{"name":"stderr","text":"VisionMamba Epoch 118/200 - Training: 100%|██████████| 1563/1563 [22:44<00:00,  1.15it/s, acc=92.4, loss=1.01]\n","output_type":"stream"},{"name":"stdout","text":"\nVisionMamba Epoch [118/200] Training Loss: 1.0148 | Accuracy: 92.42%\n","output_type":"stream"},{"name":"stderr","text":"VisionMamba Epoch 118/200 - Validation: 100%|██████████| 313/313 [01:33<00:00,  3.35it/s, acc=81.5, loss=1.22]\n","output_type":"stream"},{"name":"stdout","text":"VisionMamba Epoch [118/200] Validation Loss: 1.2161 | Accuracy: 81.45%\n\nCheckpoint saved at /kaggle/working/model_checkpoint.pth\nCheckpoint saved at epoch 118, accuracy: 81.45%\nCheckpoint saved at /kaggle/working/model_bestcheckpoint.pth\n✅ Best checkpoint saved at epoch 118, accuracy: 81.45%\n","output_type":"stream"},{"name":"stderr","text":"VisionMamba Epoch 119/200 - Training: 100%|██████████| 1563/1563 [22:43<00:00,  1.15it/s, acc=92.5, loss=1.01]\n","output_type":"stream"},{"name":"stdout","text":"\nVisionMamba Epoch [119/200] Training Loss: 1.0123 | Accuracy: 92.50%\n","output_type":"stream"},{"name":"stderr","text":"VisionMamba Epoch 119/200 - Validation: 100%|██████████| 313/313 [01:32<00:00,  3.37it/s, acc=82.8, loss=1.19]\n","output_type":"stream"},{"name":"stdout","text":"VisionMamba Epoch [119/200] Validation Loss: 1.1948 | Accuracy: 82.85%\n\nCheckpoint saved at /kaggle/working/model_checkpoint.pth\nCheckpoint saved at epoch 119, accuracy: 82.85%\nCheckpoint saved at /kaggle/working/model_bestcheckpoint.pth\n✅ Best checkpoint saved at epoch 119, accuracy: 82.85%\n","output_type":"stream"},{"name":"stderr","text":"VisionMamba Epoch 120/200 - Training: 100%|██████████| 1563/1563 [22:42<00:00,  1.15it/s, acc=92.3, loss=1.01]\n","output_type":"stream"},{"name":"stdout","text":"\nVisionMamba Epoch [120/200] Training Loss: 1.0125 | Accuracy: 92.32%\n","output_type":"stream"},{"name":"stderr","text":"VisionMamba Epoch 120/200 - Validation: 100%|██████████| 313/313 [01:32<00:00,  3.37it/s, acc=82.7, loss=1.2] \n","output_type":"stream"},{"name":"stdout","text":"VisionMamba Epoch [120/200] Validation Loss: 1.1972 | Accuracy: 82.72%\n\nCheckpoint saved at /kaggle/working/model_checkpoint.pth\nCheckpoint saved at epoch 120, accuracy: 82.72%\n","output_type":"stream"},{"name":"stderr","text":"VisionMamba Epoch 121/200 - Training: 100%|██████████| 1563/1563 [22:42<00:00,  1.15it/s, acc=92.8, loss=1.01]\n","output_type":"stream"},{"name":"stdout","text":"\nVisionMamba Epoch [121/200] Training Loss: 1.0068 | Accuracy: 92.76%\n","output_type":"stream"},{"name":"stderr","text":"VisionMamba Epoch 121/200 - Validation: 100%|██████████| 313/313 [01:32<00:00,  3.37it/s, acc=82.6, loss=1.21]\n","output_type":"stream"},{"name":"stdout","text":"VisionMamba Epoch [121/200] Validation Loss: 1.2085 | Accuracy: 82.58%\n\nCheckpoint saved at /kaggle/working/model_checkpoint.pth\nCheckpoint saved at epoch 121, accuracy: 82.58%\n","output_type":"stream"},{"name":"stderr","text":"VisionMamba Epoch 122/200 - Training: 100%|██████████| 1563/1563 [22:42<00:00,  1.15it/s, acc=92.7, loss=1.01]\n","output_type":"stream"},{"name":"stdout","text":"\nVisionMamba Epoch [122/200] Training Loss: 1.0075 | Accuracy: 92.66%\n","output_type":"stream"},{"name":"stderr","text":"VisionMamba Epoch 122/200 - Validation: 100%|██████████| 313/313 [01:32<00:00,  3.37it/s, acc=82.4, loss=1.21]\n","output_type":"stream"},{"name":"stdout","text":"VisionMamba Epoch [122/200] Validation Loss: 1.2078 | Accuracy: 82.40%\n\nCheckpoint saved at /kaggle/working/model_checkpoint.pth\nCheckpoint saved at epoch 122, accuracy: 82.40%\n","output_type":"stream"},{"name":"stderr","text":"VisionMamba Epoch 123/200 - Training: 100%|██████████| 1563/1563 [22:42<00:00,  1.15it/s, acc=92.5, loss=1.01]\n","output_type":"stream"},{"name":"stdout","text":"\nVisionMamba Epoch [123/200] Training Loss: 1.0095 | Accuracy: 92.48%\n","output_type":"stream"},{"name":"stderr","text":"VisionMamba Epoch 123/200 - Validation: 100%|██████████| 313/313 [01:32<00:00,  3.37it/s, acc=82.2, loss=1.21]\n","output_type":"stream"},{"name":"stdout","text":"VisionMamba Epoch [123/200] Validation Loss: 1.2099 | Accuracy: 82.15%\n\nCheckpoint saved at /kaggle/working/model_checkpoint.pth\nCheckpoint saved at epoch 123, accuracy: 82.15%\n","output_type":"stream"},{"name":"stderr","text":"VisionMamba Epoch 124/200 - Training:  16%|█▌        | 249/1563 [03:37<19:04,  1.15it/s, acc=92.8, loss=1]   ","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(model, test_loader, criterion, device):\n    model.eval()\n    test_loss = 0.0\n    correct = 0\n    total = 0\n\n    test_loader_tqdm = tqdm(test_loader, desc=\"Evaluating\")\n    with torch.no_grad():\n        for inputs, labels in test_loader_tqdm:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            test_loss += loss.item() * inputs.size(0)\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n\n            test_loader_tqdm.set_postfix(loss=test_loss / total, acc=100. * correct / total)\n\n    avg_loss = test_loss / len(test_loader.dataset)\n    accuracy = 100. * correct / total\n    print(f\"\\n Test Loss: {avg_loss:.4f} | Test Accuracy: {accuracy:.2f}%\")\n    return accuracy","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-02T02:58:55.130Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluate_model(VisionMamba_model, test_loader, criterion, device)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-02T02:58:55.131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport random\n\n# Hàm denormalize ảnh đã chuẩn hóa theo mean/std\ndef denormalize(img_tensor, mean, std):\n    mean = torch.tensor(mean).view(3, 1, 1)\n    std = torch.tensor(std).view(3, 1, 1)\n    return img_tensor * std + mean\n\ndef visualize_prediction(model, train_loader, device, class_names=None):\n    model.eval()  # chế độ đánh giá\n\n    # Lấy 1 batch bất kỳ\n    data_iter = iter(train_loader)\n    images, labels = next(data_iter)\n\n    # Chọn ngẫu nhiên 1 ảnh trong batch\n    idx = random.randint(0, len(images) - 1)\n    image = images[idx].unsqueeze(0).to(device)\n    label = labels[idx].item()\n\n    # Dự đoán\n    with torch.no_grad():\n        outputs = model(image)\n        _, predicted = outputs.max(1)\n        predicted = predicted.item()\n\n    # Denormalize để hiển thị đúng\n    img_denorm = denormalize(images[idx], mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n    img_np = img_denorm.permute(1, 2, 0).cpu().numpy()  # CHW → HWC\n\n    # Clamp lại để tránh giá trị vượt [0,1] do cộng trừ float\n    img_np = img_np.clip(0, 1)\n\n    # Hiển thị\n    plt.imshow(img_np)\n    plt.axis('off')\n\n    true_label = class_names[label] if class_names else str(label)\n    pred_label = class_names[predicted] if class_names else str(predicted)\n    plt.title(f\"Predicted: {pred_label} | True: {true_label}\")\n    plt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-02T02:58:55.131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n\nvisualize_prediction(VisionMamba_model, test_loader, device, class_names)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-02T02:58:55.131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nimport os\n\ndef show_confusion_matrix(model, dataloader, class_names, device):\n    model.eval()\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for inputs, labels in tqdm(dataloader, desc=\"Generating Confusion Matrix\"):\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    cm = confusion_matrix(all_labels, all_preds)\n\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=class_names, yticklabels=class_names)\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.title('Confusion Matrix')\n    plt.tight_layout()\n    plt.show()  # 🔥 Hiển thị trực tiếp thay vì lưu","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-02T02:58:55.131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_confusion_matrix(VisionMamba_model, test_loader, class_names, device)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-02T02:58:55.131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\ndef print_classification_report(model, dataloader, class_names, device):\n    model.eval()\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for inputs, labels in tqdm(dataloader, desc=\"Evaluating for Classification Report\"):\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    print()\n    print(classification_report(all_labels, all_preds, target_names=class_names, digits=2))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-02T02:58:55.131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print_classification_report(VisionMamba_model, test_loader, class_names, device)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-02T02:58:55.131Z"}},"outputs":[],"execution_count":null}]}